<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Tensorflow训练多层卷积神经网络 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="来自官方例子：Deep MNIST for Experts 在阅读之前确保自己已经理解了MNIST For ML Beginners 理解卷积 Convolution原始定义：设f,g在R上可积,定义新函数h满足以下特性： $$h(x)=(f*g)(x) =\int_{-\infty}^{+\infty} f(t)g(x-t) {\rm d}t$$ 则称h是f和g的卷积。 Wiki百科上有一个非常">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow训练多层卷积神经网络">
<meta property="og:url" content="http://yoursite.com/2017/10/28/2017-5-22-tensorflow-cnn/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="来自官方例子：Deep MNIST for Experts 在阅读之前确保自己已经理解了MNIST For ML Beginners 理解卷积 Convolution原始定义：设f,g在R上可积,定义新函数h满足以下特性： $$h(x)=(f*g)(x) =\int_{-\infty}^{+\infty} f(t)g(x-t) {\rm d}t$$ 则称h是f和g的卷积。 Wiki百科上有一个非常">
<meta property="og:image" content="http://img.blog.csdn.net/20130918153655515?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2lsZW5jZTEyMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="http://images.cnitblog.com/blog2015/678029/201504/241900156879853.png">
<meta property="og:updated_time" content="2017-10-28T03:01:44.882Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tensorflow训练多层卷积神经网络">
<meta name="twitter:description" content="来自官方例子：Deep MNIST for Experts 在阅读之前确保自己已经理解了MNIST For ML Beginners 理解卷积 Convolution原始定义：设f,g在R上可积,定义新函数h满足以下特性： $$h(x)=(f*g)(x) =\int_{-\infty}^{+\infty} f(t)g(x-t) {\rm d}t$$ 则称h是f和g的卷积。 Wiki百科上有一个非常">
<meta name="twitter:image" content="http://img.blog.csdn.net/20130918153655515?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2lsZW5jZTEyMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-2017-5-22-tensorflow-cnn" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/10/28/2017-5-22-tensorflow-cnn/" class="article-date">
  <time datetime="2017-10-28T03:11:56.873Z" itemprop="datePublished">2017-10-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Tensorflow训练多层卷积神经网络
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>来自官方例子：<a href="https://www.tensorflow.org/get_started/mnist/pros" target="_blank" rel="external">Deep MNIST for Experts</a></p>
<p>在阅读之前确保自己已经理解了<a href="https://www.tensorflow.org/get_started/mnist/beginners" target="_blank" rel="external">MNIST For ML Beginners</a></p>
<h2 id="理解卷积-Convolution"><a href="#理解卷积-Convolution" class="headerlink" title="理解卷积 Convolution"></a>理解卷积 Convolution</h2><p><strong>原始定义：</strong>设f,g在R上可积,定义新函数h满足以下特性：</p>
<p>$$h(x)=(f*g)(x) =\int_{-\infty}^{+\infty} f(t)g(x-t) {\rm d}t$$</p>
<p>则称h是f和g的卷积。</p>
<p>Wiki百科上有一个非常形象的图解来解释卷积：</p>
<blockquote>
<p>它（卷积）是其中一个函数翻转并平移后与另一个函数的乘积的积分,是一个对平移量的函数。</p>
</blockquote>
<p>这里有两个定义，翻转和平移，可以这么来理解这两个概念：</p>
<ul>
<li><strong>翻转</strong> g(t) 变成 g(-t)</li>
<li><strong>平移</strong> g(-t) 向右平移x个单位，变成g(x-t)</li>
</ul>
<p>这里就不贴图了，直接上<a href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF" target="_blank" rel="external">Wiki百科地址</a>。看下面的图解，就能理解卷积函数所谓的翻转和平移了。另外有<a href="https://graphics.stanford.edu/courses/cs178/applets/convolution.html" target="_blank" rel="external">卷积Flash演示</a>。</p>
<p>推荐一个知乎问题：<a href="https://www.zhihu.com/question/20500497" target="_blank" rel="external">在定义卷积时为什么要对其中一个函数进行翻转？</a>；这篇知乎阐述了卷积的起源。</p>
<h3 id="卷积的离散定义"><a href="#卷积的离散定义" class="headerlink" title="卷积的离散定义"></a>卷积的离散定义</h3><p>根据卷积的连续定义，很容易就能理解卷积的离散定义：</p>
<p>$$h(x)=(f*g)(x) =\sum_{k=-\infty}^{+\infty} f(k) g(x-k)  $$</p>
<p>$$s.t.(x,k\in Z)$$</p>
<p>参考总结的博文：<a href="http://mengqi92.github.io/2015/10/06/convolution/" target="_blank" rel="external">我对卷积的理解</a> 中输液的例子就能深刻理解卷积的离散定义。给出以下例子进行理解：</p>
<h3 id="二维中的卷积"><a href="#二维中的卷积" class="headerlink" title="二维中的卷积"></a>二维中的卷积</h3><p>一维的卷积搞清楚了，二维的就好说了。</p>
<p>参考总结的博文：<a href="http://mengqi92.github.io/2015/10/06/convolution/" target="_blank" rel="external">我对卷积的理解</a> 中对二维卷积的理解。</p>
<h3 id="图像处理中的卷积"><a href="#图像处理中的卷积" class="headerlink" title="图像处理中的卷积"></a>图像处理中的卷积</h3><p>参考<a href="http://blog.sina.com.cn/s/blog_4bdb170b01019atv.html" target="_blank" rel="external">图像处理（卷积）作者太棒了</a></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>理解卷积，就要理解卷积的四个核心概念：</p>
<ul>
<li>（翻转）（图像处理中很多情况不用翻转？）</li>
<li>移动</li>
<li>乘积</li>
<li>求和</li>
</ul>
<h2 id="理解卷积神经网络"><a href="#理解卷积神经网络" class="headerlink" title="理解卷积神经网络"></a>理解卷积神经网络</h2><p>参考博文：<a href="https://zhuanlan.zhihu.com/p/25868154" target="_blank" rel="external">看图判断口袋妖怪属性，学会用卷积神经网络分类（教程+代码）</a></p>
<p>推荐视频教程：<a href="http://v.youku.com/v_show/id_XMjgzNzk5Njk3Ng==.html?spm=a2h0k.8191407.0.0&amp;from=s1.8-1-1.2" target="_blank" rel="external">卷积神经网络与图像识别</a></p>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>卷积的关键理解在前面一章已经阐述过。这里讲解卷积在卷积神经网络中的使用。</p>
<p>tensorflow中卷积的写法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></div><div class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</div><div class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</div></pre></td></tr></table></figure>
<p>查看conv2d的官方解释：</p>
<blockquote>
<p>Computes a 2-D convolution given 4-D input and filter tensors.</p>
<p>Given an input tensor of shape [batch, in_height, in_width, in_channels] and a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels], this op performs the following:</p>
<ol>
<li>Flattens the filter to a 2-D matrix with shape <code>[filter_height * filter_width * in_channels, output_channels]</code>.</li>
<li>Extracts image patches from the input tensor to form a <em>virtual</em> tensor of shape <code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code>.</li>
<li>For each patch, right-multiplies the filter matrix and the image patch vector.</li>
</ol>
<h4 id="Args"><a href="#Args" class="headerlink" title="Args:"></a>Args:</h4><ul>
<li><strong>input</strong>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>. A 4-D tensor. The dimension order is interpreted according to the value of <code>data_format</code>, see below for details.</li>
<li><strong>filter</strong>: A <code>Tensor</code>. Must have the same type as <code>input</code>. A 4-D tensor of shape<code>[filter_height, filter_width, in_channels, out_channels]</code></li>
<li><strong>strides</strong>: A list of <code>ints</code>. 1-D tensor of length 4. The stride of the sliding window for each dimension of <code>input</code>. The dimension order is determined by the value of <code>data_format</code>, see below for details.</li>
<li><strong>padding</strong>: A <code>string</code> from: <code>&quot;SAME&quot;, &quot;VALID&quot;</code>. The type of padding algorithm to use.</li>
</ul>
</blockquote>
<p><strong>strides</strong> 步长（每个维度上的移动步长）默认就是[1,1,1,1]了。</p>
<p><strong>padding</strong> 填充算法，卷积核在边缘移动时，没有数据对应情况下的填充算法。</p>
<h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p>一张图就能直观解释池化：</p>
<p><img src="http://img.blog.csdn.net/20130918153655515?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2lsZW5jZTEyMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>为什么要池化？64*64的图片你算得算半天，池化可以有效提取特征并减少计算，而且池化可以防止过拟合。</p>
<h3 id="Relu激活函数"><a href="#Relu激活函数" class="headerlink" title="Relu激活函数"></a>Relu激活函数</h3><p>Relu激活函数如图所示：</p>
<p><img src="http://images.cnitblog.com/blog2015/678029/201504/241900156879853.png" alt="img"></p>
<p>图中还有一个softplus公式，公式如下：</p>
<p>$$Softplus(x) = log(1+e^x)$$</p>
<p><a href="https://www.zhihu.com/question/29021768/answer/43517930" target="_blank" rel="external">为什么ReLu要好过于tanh和sigmoid function?</a></p>
<h2 id="Tensorflow实战"><a href="#Tensorflow实战" class="headerlink" title="Tensorflow实战"></a>Tensorflow实战</h2><h3 id="tf-reshape"><a href="#tf-reshape" class="headerlink" title="tf.reshape"></a>tf.reshape</h3><p><strong>tf.reshape(tensor, shape, name=None)</strong><br>函数的作用是将tensor变换为参数shape的形式。<br>其中shape为一个列表形式，特殊的一点是列表中可以存在-1。<strong>-1代表的含义是不用我们自己指定这一维的大小</strong>，函数会自动计算，但列表中只能存在一个-1。（当然如果存在多个-1，就是一个存在多解的方程了）</p>
<p>官方例子帮助理解：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]</span></div><div class="line"><span class="comment"># tensor 't' has shape [9]</span></div><div class="line">reshape(t, [3, 3]) ==&gt; [[1, 2, 3],</div><div class="line">                        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</div><div class="line">                        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</div><div class="line"></div><div class="line"><span class="comment"># tensor 't' is [[[1, 1], [2, 2]],</span></div><div class="line"><span class="comment">#                [[3, 3], [4, 4]]]</span></div><div class="line"><span class="comment"># tensor 't' has shape [2, 2, 2]</span></div><div class="line">reshape(t, [2, 4]) ==&gt; [[1, 1, 2, 2],</div><div class="line">                        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>]]</div><div class="line"></div><div class="line"><span class="comment"># tensor 't' is [[[1, 1, 1],</span></div><div class="line"><span class="comment">#                 [2, 2, 2]],</span></div><div class="line"><span class="comment">#                [[3, 3, 3],</span></div><div class="line"><span class="comment">#                 [4, 4, 4]],</span></div><div class="line"><span class="comment">#                [[5, 5, 5],</span></div><div class="line"><span class="comment">#                 [6, 6, 6]]]</span></div><div class="line"><span class="comment"># tensor 't' has shape [3, 2, 3]</span></div><div class="line"><span class="comment"># pass '[-1]' to flatten 't'</span></div><div class="line">reshape(t, [-1]) ==&gt; [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]</div><div class="line"></div><div class="line"><span class="comment"># -1 can also be used to infer the shape</span></div><div class="line"></div><div class="line"><span class="comment"># -1 is inferred to be 9:</span></div><div class="line">reshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],</div><div class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</div><div class="line"><span class="comment"># -1 is inferred to be 2:</span></div><div class="line">reshape(t, [-1, 9]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],</div><div class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</div><div class="line"><span class="comment"># -1 is inferred to be 3:</span></div><div class="line">reshape(t, [ 2, -1, 3]) ==&gt; [[[1, 1, 1],</div><div class="line">                              [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</div><div class="line">                              [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]],</div><div class="line">                             [[<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>],</div><div class="line">                              [<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>],</div><div class="line">                              [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]]</div><div class="line"></div><div class="line"><span class="comment"># tensor 't' is [7]</span></div><div class="line"><span class="comment"># shape `[]` reshapes to a scalar</span></div><div class="line">reshape(t, []) ==&gt; 7</div></pre></td></tr></table></figure>
<h3 id="正态分布取随机值"><a href="#正态分布取随机值" class="headerlink" title="正态分布取随机值"></a>正态分布取随机值</h3><h5 id="tf-truncated-normal-shape-mean-0-0-stddev-1-0"><a href="#tf-truncated-normal-shape-mean-0-0-stddev-1-0" class="headerlink" title="tf.truncated_normal(shape,mean=0.0,stddev=1.0)"></a>tf.truncated_normal(shape,mean=0.0,stddev=1.0)</h5><p>从<strong>截断</strong>的正态分布中输出随机值。<br>生成的值服从具有指定平均值和标准偏差的正态分布，如果生成的值大于平均值2个标准偏差的值则丢弃重新选择。</p>
<p>在正态分布的曲线中，横轴区间（μ-σ，μ+σ）内的面积为68.268949%。<br>横轴区间（μ-2σ，μ+2σ）内的面积为95.449974%。<br>横轴区间（μ-3σ，μ+3σ）内的面积为99.730020%。<br>X落在（μ-3σ，μ+3σ）以外的概率小于千分之三，在实际问题中常认为相应的事件是不会发生的，基本上可以把区间（μ-3σ，μ+3σ）看作是随机变量X实际可能的取值区间，这称之为正态分布的“3σ”原则。<br>在tf.truncated_normal中如果x的取值在区间（μ-2σ，μ+2σ）之外则重新进行选择。这样保证了生成的值都在均值附近。</p>
<p>参数:</p>
<ul>
<li>shape: 一维的张量，也是输出的张量。</li>
<li>mean: 正态分布的均值。</li>
<li>stddev: 正态分布的标准差。</li>
<li>dtype: 输出的类型。</li>
<li>seed: 一个整数，当设置之后，每次生成的随机数都一样。</li>
<li>name: 操作的名字。</li>
</ul>
<h5 id="tf-random-normal-shape-mean-0-0-stddev-1-0"><a href="#tf-random-normal-shape-mean-0-0-stddev-1-0" class="headerlink" title="tf.random_normal(shape,mean=0.0,stddev=1.0)"></a>tf.random_normal(shape,mean=0.0,stddev=1.0)</h5><p>从正态分布中输出随机值。<br>参数:</p>
<ul>
<li>shape: 一维的张量，也是输出的张量。</li>
<li>mean: 正态分布的均值。</li>
<li>stddev: 正态分布的标准差。</li>
<li>dtype: 输出的类型。</li>
<li>seed: 一个整数，当设置之后，每次生成的随机数都一样。</li>
<li>name: 操作的名字。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">a = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">2</span>],seed=<span class="number">1</span>))</div><div class="line">b = tf.Variable(tf.truncated_normal([<span class="number">2</span>,<span class="number">2</span>],seed=<span class="number">2</span>))</div><div class="line">init = tf.global_variables_initializer()</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(init)</div><div class="line">    print(sess.run(a))</div><div class="line">    print(sess.run(b))</div><div class="line"></div><div class="line">输出：</div><div class="line">[[<span class="number">-0.81131822</span>  <span class="number">1.48459876</span>]</div><div class="line"> [ <span class="number">0.06532937</span> <span class="number">-2.44270396</span>]]</div><div class="line">[[<span class="number">-0.85811085</span> <span class="number">-0.19662298</span>]</div><div class="line"> [ <span class="number">0.13895047</span> <span class="number">-1.22127688</span>]]</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/10/28/2017-5-22-tensorflow-cnn/" data-id="cj9ar355v0001vfcxkb2hbwhh" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/10/28/2017-5-20-tensorflow-freshman/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Tensorflow入门笔记</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/10/28/2017-5-22-tensorflow-cnn/">Tensorflow训练多层卷积神经网络</a>
          </li>
        
          <li>
            <a href="/2017/10/28/2017-5-20-tensorflow-freshman/">Tensorflow入门笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>