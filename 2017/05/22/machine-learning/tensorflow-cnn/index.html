<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="format-detection" content="email=no"><meta name="description"><meta name="keywords" content="毛怡伟,毛线"><title>Tensorflow训练多层卷积神经网络 - 玩毛线的毛线</title><link rel="stylesheet" href="/css/main_style.min.css"><link rel="icon" href="/favicon.ico"></head><body><input id="navi" type="checkbox"><ul class="main-navication"><li><a href="/"><span>Home</span></a></li><li><a href="https://github.com"><span>Github</span></a></li><li><a href="https://www.v2ex.com/"><span>V2EX</span></a></li></ul><div class="wrapper" id="wrap"><div class="post-header"><label class="navi-button light" for="navi">MENU</label><img class="background" src="/assets/cats.jpg"><div class="post-title"><h1 class="title">Tensorflow训练多层卷积神经网络</h1><ul class="meta"><li><i class="icon icon-author"></i>玩毛线的毛线</li><li><i class="icon icon-clock"></i>16 Minutes</li><li><i class="icon icon-calendar"></i>May 22, 2017</li></ul></div></div><div class="article-content" style="max-width:800px;"><p>来自官方例子：<a href="https://www.tensorflow.org/get_started/mnist/pros" target="_blank" rel="external">Deep MNIST for Experts</a></p>
<p>在阅读之前确保自己已经理解了<a href="https://www.tensorflow.org/get_started/mnist/beginners" target="_blank" rel="external">MNIST For ML Beginners</a></p>
<h2 id="理解卷积-Convolution"><a href="#理解卷积-Convolution" class="headerlink" title="理解卷积 Convolution"></a>理解卷积 Convolution</h2><p><strong>原始定义：</strong>设f,g在R上可积,定义新函数h满足以下特性：</p>
<p>$$h(x)=(f*g)(x) =\int_{-\infty}^{+\infty} f(t)g(x-t) {\rm d}t$$</p>
<p>则称h是f和g的卷积。</p>
<p>Wiki百科上有一个非常形象的图解来解释卷积：</p>
<blockquote>
<p>它（卷积）是其中一个函数翻转并平移后与另一个函数的乘积的积分,是一个对平移量的函数。</p>
</blockquote>
<p>这里有两个定义，翻转和平移，可以这么来理解这两个概念：</p>
<ul>
<li><strong>翻转</strong> g(t) 变成 g(-t)</li>
<li><strong>平移</strong> g(-t) 向右平移x个单位，变成g(x-t)</li>
</ul>
<p>这里就不贴图了，直接上<a href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF" target="_blank" rel="external">Wiki百科地址</a>。看下面的图解，就能理解卷积函数所谓的翻转和平移了。另外有<a href="https://graphics.stanford.edu/courses/cs178/applets/convolution.html" target="_blank" rel="external">卷积Flash演示</a>。</p>
<p>推荐一个知乎问题：<a href="https://www.zhihu.com/question/20500497" target="_blank" rel="external">在定义卷积时为什么要对其中一个函数进行翻转？</a>；这篇知乎阐述了卷积的起源。</p>
<h3 id="卷积的离散定义"><a href="#卷积的离散定义" class="headerlink" title="卷积的离散定义"></a>卷积的离散定义</h3><p>根据卷积的连续定义，很容易就能理解卷积的离散定义：</p>
<p>$$h(x)=(f*g)(x) =\sum_{k=-\infty}^{+\infty} f(k) g(x-k)  $$</p>
<p>$$s.t.(x,k\in Z)$$</p>
<p>参考总结的博文：<a href="http://mengqi92.github.io/2015/10/06/convolution/" target="_blank" rel="external">我对卷积的理解</a> 中输液的例子就能深刻理解卷积的离散定义。给出以下例子进行理解：</p>
<h3 id="二维中的卷积"><a href="#二维中的卷积" class="headerlink" title="二维中的卷积"></a>二维中的卷积</h3><p>一维的卷积搞清楚了，二维的就好说了。</p>
<p>参考总结的博文：<a href="http://mengqi92.github.io/2015/10/06/convolution/" target="_blank" rel="external">我对卷积的理解</a> 中对二维卷积的理解。</p>
<h3 id="图像处理中的卷积"><a href="#图像处理中的卷积" class="headerlink" title="图像处理中的卷积"></a>图像处理中的卷积</h3><p>参考<a href="http://blog.sina.com.cn/s/blog_4bdb170b01019atv.html" target="_blank" rel="external">图像处理（卷积）作者太棒了</a></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>理解卷积，就要理解卷积的四个核心概念：</p>
<ul>
<li>（翻转）（图像处理中很多情况不用翻转？）</li>
<li>移动</li>
<li>乘积</li>
<li>求和</li>
</ul>
<h2 id="理解卷积神经网络"><a href="#理解卷积神经网络" class="headerlink" title="理解卷积神经网络"></a>理解卷积神经网络</h2><p>参考博文：<a href="https://zhuanlan.zhihu.com/p/25868154" target="_blank" rel="external">看图判断口袋妖怪属性，学会用卷积神经网络分类（教程+代码）</a></p>
<p>推荐视频教程：<a href="http://v.youku.com/v_show/id_XMjgzNzk5Njk3Ng==.html?spm=a2h0k.8191407.0.0&amp;from=s1.8-1-1.2" target="_blank" rel="external">卷积神经网络与图像识别</a></p>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>卷积的关键理解在前面一章已经阐述过。这里讲解卷积在卷积神经网络中的使用。</p>
<p>tensorflow中卷积的写法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></div><div class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</div><div class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</div></pre></td></tr></table></figure>
<p>查看conv2d的官方解释：</p>
<blockquote>
<p>Computes a 2-D convolution given 4-D input and filter tensors.</p>
<p>Given an input tensor of shape [batch, in_height, in_width, in_channels] and a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels], this op performs the following:</p>
<ol>
<li>Flattens the filter to a 2-D matrix with shape <code>[filter_height * filter_width * in_channels, output_channels]</code>.</li>
<li>Extracts image patches from the input tensor to form a <em>virtual</em> tensor of shape <code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code>.</li>
<li>For each patch, right-multiplies the filter matrix and the image patch vector.</li>
</ol>
<h4 id="Args"><a href="#Args" class="headerlink" title="Args:"></a>Args:</h4><ul>
<li><strong>input</strong>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>. A 4-D tensor. The dimension order is interpreted according to the value of <code>data_format</code>, see below for details.</li>
<li><strong>filter</strong>: A <code>Tensor</code>. Must have the same type as <code>input</code>. A 4-D tensor of shape<code>[filter_height, filter_width, in_channels, out_channels]</code></li>
<li><strong>strides</strong>: A list of <code>ints</code>. 1-D tensor of length 4. The stride of the sliding window for each dimension of <code>input</code>. The dimension order is determined by the value of <code>data_format</code>, see below for details.</li>
<li><strong>padding</strong>: A <code>string</code> from: <code>&quot;SAME&quot;, &quot;VALID&quot;</code>. The type of padding algorithm to use.</li>
</ul>
</blockquote>
<p><strong>strides</strong> 步长（每个维度上的移动步长）默认就是[1,1,1,1]了。</p>
<p><strong>padding</strong> 填充算法，卷积核在边缘移动时，没有数据对应情况下的填充算法。</p>
<h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p>一张图就能直观解释池化：</p>
<p><img src="http://img.blog.csdn.net/20130918153655515?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2lsZW5jZTEyMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>为什么要池化？64*64的图片你算得算半天，池化可以有效提取特征并减少计算，而且池化可以防止过拟合。</p>
<h3 id="Relu激活函数"><a href="#Relu激活函数" class="headerlink" title="Relu激活函数"></a>Relu激活函数</h3><p>Relu激活函数如图所示：</p>
<p><img src="http://images.cnitblog.com/blog2015/678029/201504/241900156879853.png" alt="img"></p>
<p>图中还有一个softplus公式，公式如下：</p>
<p>$$Softplus(x) = log(1+e^x)$$</p>
<p><a href="https://www.zhihu.com/question/29021768/answer/43517930" target="_blank" rel="external">为什么ReLu要好过于tanh和sigmoid function?</a></p>
<h2 id="Tensorflow实战"><a href="#Tensorflow实战" class="headerlink" title="Tensorflow实战"></a>Tensorflow实战</h2><h3 id="tf-reshape"><a href="#tf-reshape" class="headerlink" title="tf.reshape"></a>tf.reshape</h3><p><strong>tf.reshape(tensor, shape, name=None)</strong><br>函数的作用是将tensor变换为参数shape的形式。<br>其中shape为一个列表形式，特殊的一点是列表中可以存在-1。<strong>-1代表的含义是不用我们自己指定这一维的大小</strong>，函数会自动计算，但列表中只能存在一个-1。（当然如果存在多个-1，就是一个存在多解的方程了）</p>
<p>官方例子帮助理解：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]</span></div><div class="line"><span class="comment"># tensor 't' has shape [9]</span></div><div class="line">reshape(t, [3, 3]) ==&gt; [[1, 2, 3],</div><div class="line">                        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</div><div class="line">                        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</div><div class="line"></div><div class="line"><span class="comment"># tensor 't' is [[[1, 1], [2, 2]],</span></div><div class="line"><span class="comment">#                [[3, 3], [4, 4]]]</span></div><div class="line"><span class="comment"># tensor 't' has shape [2, 2, 2]</span></div><div class="line">reshape(t, [2, 4]) ==&gt; [[1, 1, 2, 2],</div><div class="line">                        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>]]</div><div class="line"></div><div class="line"><span class="comment"># tensor 't' is [[[1, 1, 1],</span></div><div class="line"><span class="comment">#                 [2, 2, 2]],</span></div><div class="line"><span class="comment">#                [[3, 3, 3],</span></div><div class="line"><span class="comment">#                 [4, 4, 4]],</span></div><div class="line"><span class="comment">#                [[5, 5, 5],</span></div><div class="line"><span class="comment">#                 [6, 6, 6]]]</span></div><div class="line"><span class="comment"># tensor 't' has shape [3, 2, 3]</span></div><div class="line"><span class="comment"># pass '[-1]' to flatten 't'</span></div><div class="line">reshape(t, [-1]) ==&gt; [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]</div><div class="line"></div><div class="line"><span class="comment"># -1 can also be used to infer the shape</span></div><div class="line"></div><div class="line"><span class="comment"># -1 is inferred to be 9:</span></div><div class="line">reshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],</div><div class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</div><div class="line"><span class="comment"># -1 is inferred to be 2:</span></div><div class="line">reshape(t, [-1, 9]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],</div><div class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</div><div class="line"><span class="comment"># -1 is inferred to be 3:</span></div><div class="line">reshape(t, [ 2, -1, 3]) ==&gt; [[[1, 1, 1],</div><div class="line">                              [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</div><div class="line">                              [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]],</div><div class="line">                             [[<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>],</div><div class="line">                              [<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>],</div><div class="line">                              [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]]</div><div class="line"></div><div class="line"><span class="comment"># tensor 't' is [7]</span></div><div class="line"><span class="comment"># shape `[]` reshapes to a scalar</span></div><div class="line">reshape(t, []) ==&gt; 7</div></pre></td></tr></table></figure>
<h3 id="正态分布取随机值"><a href="#正态分布取随机值" class="headerlink" title="正态分布取随机值"></a>正态分布取随机值</h3><h5 id="tf-truncated-normal-shape-mean-0-0-stddev-1-0"><a href="#tf-truncated-normal-shape-mean-0-0-stddev-1-0" class="headerlink" title="tf.truncated_normal(shape,mean=0.0,stddev=1.0)"></a>tf.truncated_normal(shape,mean=0.0,stddev=1.0)</h5><p>从<strong>截断</strong>的正态分布中输出随机值。<br>生成的值服从具有指定平均值和标准偏差的正态分布，如果生成的值大于平均值2个标准偏差的值则丢弃重新选择。</p>
<p>在正态分布的曲线中，横轴区间（μ-σ，μ+σ）内的面积为68.268949%。<br>横轴区间（μ-2σ，μ+2σ）内的面积为95.449974%。<br>横轴区间（μ-3σ，μ+3σ）内的面积为99.730020%。<br>X落在（μ-3σ，μ+3σ）以外的概率小于千分之三，在实际问题中常认为相应的事件是不会发生的，基本上可以把区间（μ-3σ，μ+3σ）看作是随机变量X实际可能的取值区间，这称之为正态分布的“3σ”原则。<br>在tf.truncated_normal中如果x的取值在区间（μ-2σ，μ+2σ）之外则重新进行选择。这样保证了生成的值都在均值附近。</p>
<p>参数:</p>
<ul>
<li>shape: 一维的张量，也是输出的张量。</li>
<li>mean: 正态分布的均值。</li>
<li>stddev: 正态分布的标准差。</li>
<li>dtype: 输出的类型。</li>
<li>seed: 一个整数，当设置之后，每次生成的随机数都一样。</li>
<li>name: 操作的名字。</li>
</ul>
<h5 id="tf-random-normal-shape-mean-0-0-stddev-1-0"><a href="#tf-random-normal-shape-mean-0-0-stddev-1-0" class="headerlink" title="tf.random_normal(shape,mean=0.0,stddev=1.0)"></a>tf.random_normal(shape,mean=0.0,stddev=1.0)</h5><p>从正态分布中输出随机值。<br>参数:</p>
<ul>
<li>shape: 一维的张量，也是输出的张量。</li>
<li>mean: 正态分布的均值。</li>
<li>stddev: 正态分布的标准差。</li>
<li>dtype: 输出的类型。</li>
<li>seed: 一个整数，当设置之后，每次生成的随机数都一样。</li>
<li>name: 操作的名字。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">a = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">2</span>],seed=<span class="number">1</span>))</div><div class="line">b = tf.Variable(tf.truncated_normal([<span class="number">2</span>,<span class="number">2</span>],seed=<span class="number">2</span>))</div><div class="line">init = tf.global_variables_initializer()</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(init)</div><div class="line">    print(sess.run(a))</div><div class="line">    print(sess.run(b))</div><div class="line"></div><div class="line">输出：</div><div class="line">[[<span class="number">-0.81131822</span>  <span class="number">1.48459876</span>]</div><div class="line"> [ <span class="number">0.06532937</span> <span class="number">-2.44270396</span>]]</div><div class="line">[[<span class="number">-0.85811085</span> <span class="number">-0.19662298</span>]</div><div class="line"> [ <span class="number">0.13895047</span> <span class="number">-1.22127688</span>]]</div></pre></td></tr></table></figure>
</div><div class="article-meta" style="max-width:800px;"></div><div class="article-comment" style="max-width:800px;"><div class="ds-thread" id="ds-thread" data-thread-key="cj9azsjkw0006sacxzhtmejbw" data-title="Tensorflow训练多层卷积神经网络" data-url="http://yoursite.com/2017/05/22/machine-learning/tensorflow-cnn/" site-name="ueibo"></div><script>var siteName = document.getElementById('ds-thread').getAttribute('site-name');
var duoshuoQuery = {short_name: siteName};
(function() {
  var ds = document.createElement('script');
  ds.type = 'text/javascript';ds.async = true;
  ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
  ds.charset = 'UTF-8';
  (document.getElementsByTagName('head')[0] 
  || document.getElementsByTagName('body')[0]).appendChild(ds);
})();</script></div><ul class="navication"><li class="home"><a href="/"><i class="icon icon-home"></i></a></li><li><a href="/2017/06/01/computer-science/c++ overload-and-virtual-function/"><i class="icon icon-arror-left"></i></a></li><li><a href="/2017/05/20/machine-learning/tensorflow-freshman/"><i class="icon icon-arror-right"></i></a></li></ul><div class="page-footer"><div class="top"><ul class="social"><li><a href="https://github.com/sanmaopep" title="Github" target="_blank"><i class="icon icon-github"></i></a></li></ul></div><div class="bottom"><p class="copyright">© 2017 玩毛线的毛线<br><small>POWER BY <a href="https://hexo.io" target="_blank">HEXO</a></small><small>, THEME BY <a href="https://github.com/BoizZ/hexo-theme-laughing" target="_blank">LAUGHING</a></small></p></div></div></div><script>var wrap = document.getElementById('wrap');
window.onload = function () {
  wrap.className += ' done';
}</script></body></html>